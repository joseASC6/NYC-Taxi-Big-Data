{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "from io import BytesIO\n",
    "from datetime import datetime, date\n",
    "import holidays\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, isnan, when, count, udf, to_date, year, month, date_format, size, split, dayofweek\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, MinMaxScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'my-bigdataproject-jg'\n",
    "gs_path  = f'gs://{bucket_name}/'\n",
    "cleaned_folder = 'cleaned/'\n",
    "destination_folder = 'code_and_models/'\n",
    "\n",
    "storage_client = storage.Client() \n",
    "bucket = storage_client.get_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "    .appName(\"Taxi Demand Prediction\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_files = gs_path + cleaned_folder + 'weather_data_*.parquet'\n",
    "weather_df = spark.read.parquet(weather_files)\n",
    "weather_df.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_files = gs_path + cleaned_folder + \"cleaned_yellow_tripdata_2023-??.parquet\"\n",
    "taxi_df = spark.read.parquet(taxi_files)\n",
    "taxi_df.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df = taxi_df.drop('__index_level_0__')\n",
    "taxi_df = taxi_df.drop('dropoff_datetime')\n",
    "taxi_df = taxi_df.drop('DOLocationID')\n",
    "taxi_df = taxi_df.withColumn('pickup_datetime', to_date(col('pickup_datetime')))\n",
    "\n",
    "taxi_df.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone_file = gs_path + cleaned_folder + 'taxi_zones_data.csv'\n",
    "taxi_zone_df = spark.read.parquet(taxi_zone_file)\n",
    "\n",
    "taxi_df.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df = taxi_df.join(taxi_zone_df, taxi_df.PULocationID == taxi_zone_df.LocationID)\n",
    "taxi_df = taxi_df.drop('zone')\n",
    "taxi_df = taxi_df.drop('PULocationID')\n",
    "taxi_df = taxi_df.drop('LocationID')\n",
    "taxi_df.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df.createOrReplaceTempView('taxi_df')\n",
    "taxi_df = spark.sql('SELECT pickup_datetime, borough, COUNT(*) as total_trips FROM taxi_df GROUP BY pickup_datetime, borough')\n",
    "taxi_df = taxi_df.dropna(subset=['total_trips'])\n",
    "taxi_df = taxi_df.filter(taxi_df.borough != 'EWR')\n",
    "taxi_df.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = weather_df.withColumnRenamed('borough', 'weather_borough')\n",
    "\n",
    "combined_df = taxi_df.join(weather_df, [taxi_df.pickup_datetime == weather_df.datetime, taxi_df.borough == weather_df.weather_borough])\n",
    "\n",
    "combined_df = combined_df.drop('pickup_datetime').\n",
    "combined_df = combined_df.drop('weather_borough')\n",
    "\n",
    "combined_df.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Feature Engineering\n",
    "Original column - \n",
    "\n",
    "Features:\n",
    "\n",
    "datetime\n",
    "\n",
    "Features:\n",
    "Month of the Year\n",
    "Day of the Week \n",
    "Weekend\n",
    "Holiday\n",
    "\n",
    "borough - StringIndexer\n",
    "\n",
    "temp\n",
    " - Use the lowest tempmin value and highest tempmax value to create a range for min max scaling\n",
    " - MinMaxScaler\n",
    "\n",
    "feels_like\n",
    "- Use the lowest tempmin value and highest tempmax value to create a range for min max scaling\n",
    "- MinMaxScaler\n",
    "\n",
    "use min max scaling for the following columns:\n",
    "humidity\n",
    "precip\n",
    "snow\n",
    "snowdepth\n",
    "windspeed\n",
    "cloudcover\n",
    "visibility\n",
    "\n",
    "uvindex - Encode directly as integer\n",
    "\n",
    "conditions\n",
    " - Conditions has these options Clear, Snow, Overcast, Rain, Partially cloudy, Overcast\n",
    " - Some records have these options joint, for example: Snow, Rain, Overcast |      Rain, Overcast\n",
    " - split these options into separate columns and encode them as binary values\n",
    "\"\"\"\n",
    "# datetime\n",
    "combined_df = combined_df.withColumn('month', month(col('datetime')))\n",
    "combined_df = combined_df.withColumn('dayofweek', dayofweek(col('datetime')))\n",
    "combined_df = combined_df.withColumn('weekend', when(col('dayofweek') == 1, 1.0).when(col('dayofweek') == 7, 1.0).otherwise(0))\n",
    "\n",
    "# temp\n",
    "\n",
    "temp_min = combined_df.agg(F.min('tempmin')).collect()[0][0]\n",
    "temp_max = combined_df.agg(F.max('tempmax')).collect()[0][0]\n",
    "\n",
    "temp_assembler = VectorAssembler(inputCols=['temp'], outputCol='temp_vector')\n",
    "temp_scaler = MinMaxScaler(inputCol='temp_vector', outputCol='temp_scaled', min=temp_min, max=temp_max)\n",
    "\n",
    "# feels_like\n",
    "feelslike_min = combined_df.agg(F.min('feelslikemin')).collect()[0][0]\n",
    "feelslike_max = combined_df.agg(F.max('feelslikemax')).collect()[0][0]\n",
    "\n",
    "feelslike_assembler = VectorAssembler(inputCols=['feelslike'], outputCol='feelslike_vector')\n",
    "feelslike_scaler = MinMaxScaler(inputCol='feelslike_vector', outputCol='feelslike_scaled', min=feelslike_min, max=feelslike_max)\n",
    "\n",
    "# humidity\n",
    "humidity_assembler = VectorAssembler(inputCols=['humidity'], outputCol='humidity_vector')\n",
    "humidity_scaler = MinMaxScaler(inputCol='humidity_vector', outputCol='humidity_scaled')\n",
    "\n",
    "# precip\n",
    "precip_assembler = VectorAssembler(inputCols=['precip'], outputCol='precip_vector')\n",
    "precip_scaler = MinMaxScaler(inputCol='precip_vector', outputCol='precip_scaled')\n",
    "\n",
    "# snow\n",
    "snow_assembler = VectorAssembler(inputCols=['snow'], outputCol='snow_vector')\n",
    "snow_scaler = MinMaxScaler(inputCol='snow_vector', outputCol='snow_scaled')\n",
    "\n",
    "# snowdepth\n",
    "snowdepth_assembler = VectorAssembler(inputCols=['snowdepth'], outputCol='snowdepth_vector')\n",
    "snowdepth_scaler = MinMaxScaler(inputCol='snowdepth_vector', outputCol='snowdepth_scaled')\n",
    "\n",
    "# windspeed\n",
    "windspeed_assembler = VectorAssembler(inputCols=['windspeed'], outputCol='windspeed_vector')\n",
    "windspeed_scaler = MinMaxScaler(inputCol='windspeed_vector', outputCol='windspeed_scaled')\n",
    "\n",
    "# cloudcover\n",
    "cloudcover_assembler = VectorAssembler(inputCols=['cloudcover'], outputCol='cloudcover_vector')\n",
    "cloudcover_scaler = MinMaxScaler(inputCol='cloudcover_vector', outputCol='cloudcover_scaled')\n",
    "\n",
    "# visibility\n",
    "visibility_assembler = VectorAssembler(inputCols=['visibility'], outputCol='visibility_vector')\n",
    "visibility_scaler = MinMaxScaler(inputCol='visibility_vector', outputCol='visibility_scaled')\n",
    "\n",
    "# uvindex\n",
    "\n",
    "# conditions\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol='borough', outputCol='borough_index')\n",
    "encoder = OneHotEncoder(inputCols=['borough_index', 'month', 'dayofweek', 'weekend'], outputCols=['borough_Vector', 'month_Vector', 'dayofweek_Vector', 'weekend_Vector'])\n",
    "assembler = VectorAssembler(inputCols=['month_Vector', 'dayofweek_Vector', 'weekend_Vector', 'borough_Vector', 'temp_scaled', 'feelslike_scaled', 'humidity_scaled', 'precip_scaled', 'snow_scaled', 'snowdepth_scaled', 'windspeed_scaled', 'cloudcover_scaled', 'visibility_scaled', 'uvindex'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new frame with the transformed data\n",
    "pipeline = Pipeline(stages=[temp_assembler, temp_scaler, feelslike_assembler, feelslike_scaler, humidity_assembler, humidity_scaler, precip_assembler, precip_scaler, snow_assembler, snow_scaler, snowdepth_assembler, snowdepth_scaler, windspeed_assembler, windspeed_scaler, cloudcover_assembler, cloudcover_scaler, visibility_assembler, visibility_scaler, indexer, encoder, assembler])\n",
    "model = pipeline.fit(combined_df)\n",
    "transformed_df = model.transform(combined_df)\n",
    "\n",
    "# Save the transformed data\n",
    "transformed_df.write.parquet(gs_path + destination_folder + 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = combined_df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "linear_reg = LinearRegression(labelCol='total_trips')\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='total_trips')\n",
    "\n",
    "regresion_pipe = Pipeline(stages=[indexer, temp_assembler, temp_scaler, feelslike_assembler, feelslike_scaler, humidity_assembler, humidity_scaler, precip_assembler, precip_scaler, snow_assembler, snow_scaler, snowdepth_assembler, snowdepth_scaler, windspeed_assembler, windspeed_scaler, cloudcover_assembler, cloudcover_scaler, visibility_assembler, visibility_scaler, encoder, assembler, linear_reg])\n",
    "\n",
    "grid = ParamGridBuilder()\n",
    "grid = grid.build()\n",
    "\n",
    "cv = CrossValidator(estimator=regresion_pipe, estimatorParamMaps=grid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "all_models = cv.fit(train_df)\n",
    "\n",
    "print(f\"Average metrics: {all_models.avgMetrics}\")\n",
    "\n",
    "best_model = all_models.bestModel\n",
    "\n",
    "test_results = best_model.transform(test_df)\n",
    "\n",
    "rmse = evaluator.evaluate(test_results, {evaluator.metricName: \"rmse\"})\n",
    "mse = evaluator.evaluate(test_results, {evaluator.metricName: \"mse\"})\n",
    "mae = evaluator.evaluate(test_results, {evaluator.metricName: \"mae\"})\n",
    "r2 = evaluator.evaluate(test_results, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R2: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = best_model.stages[-1].coefficients\n",
    "print(\"bestModel coefficients\", coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_path = gs_path + destination_folder + 'model'\n",
    "best_model.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol='borough', outputCol='borough_index')\n",
    "encoder = OneHotEncoder(inputCol='borough_index', outputCol='borough_encoded')\n",
    "assembler = VectorAssembler(inputCols=['month', 'dayofweek', 'weekend', 'borough_encoded', 'temp_scaled', 'feelslike_scaled', 'humidity_scaled', 'precip_scaled', 'snow_scaled', 'snowdepth_scaled', 'windspeed_scaled', 'cloudcover_scaled', 'visibility_scaled', 'uvindex'], outputCol='features')\n",
    "\n",
    "train_df, test_df = combined_df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "linear_reg = LinearRegression(labelCol='total_trips')\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='total_trips')\n",
    "\n",
    "regresion_pipe = Pipeline(stages=[indexer, encoder, temp_assembler, temp_scaler, feelslike_assembler, feelslike_scaler, humidity_assembler, humidity_scaler, precip_assembler, precip_scaler, snow_assembler, snow_scaler, snowdepth_assembler, snowdepth_scaler, windspeed_assembler, windspeed_scaler, cloudcover_assembler, cloudcover_scaler, visibility_assembler, visibility_scaler, assembler, linear_reg])\n",
    "\n",
    "grid = ParamGridBuilder()\n",
    "grid = grid.build()\n",
    "cv = CrossValidator(estimator=regresion_pipe, estimatorParamMaps=grid, evaluator=evaluator, numFolds=3)\n",
    "all_models = cv.fit(train_df)\n",
    "\n",
    "print(f\"Average metrics: {all_models.avgMetrics}\")\n",
    "\n",
    "best_model = all_models.bestModel\n",
    "\n",
    "test_results = best_model.transform(test_df)\n",
    "\n",
    "rmse = evaluator.evaluate(test_results, {evaluator.metricName: \"rmse\"})\n",
    "mse = evaluator.evaluate(test_results, {evaluator.metricName: \"mse\"})\n",
    "mae = evaluator.evaluate(test_results, {evaluator.metricName: \"mae\"})\n",
    "r2 = evaluator.evaluate(test_results, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R2: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol='borough', outputCol='borough_index')\n",
    "encoder = OneHotEncoder(inputCol='borough_index', outputCol='borough_encoded')\n",
    "assembler = VectorAssembler(inputCols=['month', 'dayofweek', 'weekend', 'borough_encoded', 'temp_scaled', 'feelslike_scaled', 'humidity_scaled', 'precip_scaled', 'snow_scaled', 'snowdepth_scaled', 'windspeed_scaled', 'cloudcover_scaled', 'visibility_scaled', 'uvindex'], outputCol='features')\n",
    "\n",
    "pipeline = Pipeline(stages=[temp_assembler, temp_scaler, feelslike_assembler, feelslike_scaler, humidity_assembler, humidity_scaler, precip_assembler, precip_scaler, snow_assembler, snow_scaler, snowdepth_assembler, snowdepth_scaler, windspeed_assembler, windspeed_scaler, cloudcover_assembler, cloudcover_scaler, visibility_assembler, visibility_scaler, indexer, encoder, assembler])\n",
    "\n",
    "pipeline_model = pipeline.fit(combined_df).transform(combined_df)\n",
    "\n",
    "train_df, test_df = pipeline_model.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "lr = LinearRegression(featuresCol='features', labelCol='total_trips')\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "lr_predictions = lr_model.transform(test_df)\n",
    "lr_evaluator = RegressionEvaluator(predictionCol='prediction', labelCol='total_trips')\n",
    "\n",
    "rmse = lr_evaluator.evaluate(lr_predictions, {lr_evaluator.metricName: 'rmse'})\n",
    "mae = lr_evaluator.evaluate(lr_predictions, {lr_evaluator.metricName: 'mae'})\n",
    "mse = lr_evaluator.evaluate(lr_predictions, {lr_evaluator.metricName: 'mse'})\n",
    "r2 = lr_evaluator.evaluate(lr_predictions, {lr_evaluator.metricName: 'r2'})\n",
    "\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'MSE: {mse}')\n",
    "print(f'R2: {r2}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
