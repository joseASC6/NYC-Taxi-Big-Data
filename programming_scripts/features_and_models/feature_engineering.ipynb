{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "from io import BytesIO\n",
    "from datetime import datetime, date\n",
    "import holidays\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, year, month, dayofweek, hour\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, MinMaxScaler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'my-bigdataproject-jg'\n",
    "gs_path  = f'gs://{bucket_name}/'\n",
    "cleaned_folder = 'cleaned/'\n",
    "destination_folder = 'code_and_models/'\n",
    "\n",
    "storage_client = storage.Client() \n",
    "bucket = storage_client.get_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "    .appName(\"Taxi Demand Prediction\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_files = gs_path + cleaned_folder + 'weather_data_*.parquet'\n",
    "weather_df = spark.read.parquet(weather_files)\n",
    "weather_df.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_files = gs_path + cleaned_folder + \"cleaned_yellow_tripdata_2023-??.parquet\"\n",
    "taxi_df = spark.read.parquet(taxi_files)\n",
    "taxi_df.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df = taxi_df.drop('__index_level_0__')\n",
    "taxi_df = taxi_df.drop('dropoff_datetime')\n",
    "taxi_df = taxi_df.drop('DOLocationID')\n",
    "taxi_df = taxi_df.withColumn('pickup_datetime', to_date(col('pickup_datetime')))\n",
    "\n",
    "taxi_df.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone_file = gs_path + cleaned_folder + 'taxi_zones_data.csv'\n",
    "taxi_zone_df = spark.read.parquet(taxi_zone_file)\n",
    "\n",
    "taxi_df.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df = taxi_df.join(taxi_zone_df, taxi_df.PULocationID == taxi_zone_df.LocationID)\n",
    "taxi_df = taxi_df.drop('zone')\n",
    "taxi_df = taxi_df.drop('PULocationID')\n",
    "taxi_df = taxi_df.drop('LocationID')\n",
    "taxi_df.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df.createOrReplaceTempView('taxi_df')\n",
    "taxi_df = spark.sql('SELECT pickup_datetime, borough, COUNT(*) as total_trips FROM taxi_df GROUP BY pickup_datetime, borough')\n",
    "taxi_df = taxi_df.dropna(subset=['total_trips'])\n",
    "taxi_df = taxi_df.filter(taxi_df.borough != 'EWR')\n",
    "taxi_df.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = weather_df.withColumnRenamed('borough', 'weather_borough')\n",
    "\n",
    "combined_df = taxi_df.join(weather_df, [taxi_df.pickup_datetime == weather_df.datetime, taxi_df.borough == weather_df.weather_borough])\n",
    "\n",
    "combined_df = combined_df.drop('pickup_datetime').\n",
    "combined_df = combined_df.drop('weather_borough')\n",
    "\n",
    "combined_df.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Feature Engineering\n",
    "Original column - \n",
    "\n",
    "Features:\n",
    "\n",
    "datetime\n",
    "\n",
    "Features:\n",
    "Month of the Year\n",
    "Day of the Week \n",
    "Weekend\n",
    "Holiday\n",
    "\n",
    "borough - StringIndexer\n",
    "\n",
    "temp\n",
    " - Use the lowest tempmin value and highest tempmax value to create a range for min max scaling\n",
    " - MinMaxScaler\n",
    "\n",
    "feels_like\n",
    "- Use the lowest tempmin value and highest tempmax value to create a range for min max scaling\n",
    "- MinMaxScaler\n",
    "\n",
    "use min max scaling for the following columns:\n",
    "humidity\n",
    "precip\n",
    "snow\n",
    "snowdepth\n",
    "windspeed\n",
    "cloudcover\n",
    "visibility\n",
    "\n",
    "uvindex - Encode directly as integer\n",
    "\n",
    "conditions\n",
    " - Conditions has these options Clear, Snow, Overcast, Rain, Partially cloudy, Overcast\n",
    " - Some records have these options joint, for example: Snow, Rain, Overcast |      Rain, Overcast\n",
    " - split these options into separate columns and encode them as binary values\n",
    "\"\"\"\n",
    "\n",
    "# datetime\n",
    "combined_df = combined_df.withColumn('month', month(col('datetime')))\n",
    "combined_df = combined_df.withColumn('dayofweek', dayofweek(col('datetime')))\n",
    "combined_df = combined_df.withColumn('weekend', when(col('dayofweek') == 1, 1.0).when(col('dayofweek') == 7, 1.0).otherwise(0))\n",
    "\n",
    "# borough\n",
    "indexer = StringIndexer(inputCol='borough', outputCol='borough_index')\n",
    "encoder = OneHotEncoder(inputCol='borough_index', outputCol='borough_Vector', dropLast=False)\n",
    "\n",
    "# temp\n",
    "\n",
    "temp_min = combined_df.agg(F.min('tempmin')).collect()[0][0]\n",
    "temp_max = combined_df.agg(F.max('tempmax')).collect()[0][0]\n",
    "\n",
    "temp_assembler = VectorAssembler(inputCols=['temp'], outputCol='temp_vector')\n",
    "temp_scaler = MinMaxScaler(inputCol='temp_vector', outputCol='temp_scaled', min=temp_min, max=temp_max)\n",
    "\n",
    "# feels_like\n",
    "feelslike_min = combined_df.agg(F.min('feelslikemin')).collect()[0][0]\n",
    "feelslike_max = combined_df.agg(F.max('feelslikemax')).collect()[0][0]\n",
    "\n",
    "feelslike_assembler = VectorAssembler(inputCols=['feelslike'], outputCol='feelslike_vector')\n",
    "feelslike_scaler = MinMaxScaler(inputCol='feelslike_vector', outputCol='feelslike_scaled', min=feelslike_min, max=feelslike_max)\n",
    "\n",
    "# humidity\n",
    "humidity_assembler = VectorAssembler(inputCols=['humidity'], outputCol='humidity_vector')\n",
    "humidity_scaler = MinMaxScaler(inputCol='humidity_vector', outputCol='humidity_scaled')\n",
    "\n",
    "# precip\n",
    "precip_assembler = VectorAssembler(inputCols=['precip'], outputCol='precip_vector')\n",
    "precip_scaler = MinMaxScaler(inputCol='precip_vector', outputCol='precip_scaled')\n",
    "\n",
    "# snow\n",
    "snow_assembler = VectorAssembler(inputCols=['snow'], outputCol='snow_vector')\n",
    "snow_scaler = MinMaxScaler(inputCol='snow_vector', outputCol='snow_scaled')\n",
    "\n",
    "# snowdepth\n",
    "snowdepth_assembler = VectorAssembler(inputCols=['snowdepth'], outputCol='snowdepth_vector')\n",
    "snowdepth_scaler = MinMaxScaler(inputCol='snowdepth_vector', outputCol='snowdepth_scaled')\n",
    "\n",
    "# windspeed\n",
    "windspeed_assembler = VectorAssembler(inputCols=['windspeed'], outputCol='windspeed_vector')\n",
    "windspeed_scaler = MinMaxScaler(inputCol='windspeed_vector', outputCol='windspeed_scaled')\n",
    "\n",
    "# cloudcover\n",
    "cloudcover_assembler = VectorAssembler(inputCols=['cloudcover'], outputCol='cloudcover_vector')\n",
    "cloudcover_scaler = MinMaxScaler(inputCol='cloudcover_vector', outputCol='cloudcover_scaled')\n",
    "\n",
    "# visibility\n",
    "visibility_assembler = VectorAssembler(inputCols=['visibility'], outputCol='visibility_vector')\n",
    "visibility_scaler = MinMaxScaler(inputCol='visibility_vector', outputCol='visibility_scaled')\n",
    "\n",
    "\n",
    "# skip conditions for now\n",
    "\n",
    "\"\"\"# encode the new features\n",
    "combined_df = combined_df.drop('tempmax')\n",
    "combined_df = combined_df.drop('tempmin')\n",
    "combined_df = combined_df.drop('feelslikemax')\n",
    "combined_df = combined_df.drop('feelslikemin')\n",
    "combined_df = combined_df.drop('preciptype')\n",
    "combined_df = combined_df.drop('conditions')\"\"\"\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['month', 'dayofweek', 'weekend', 'borough_Vector', 'temp_scaled', 'feelslike_scaled', 'humidity_scaled', 'precip_scaled', 'snow_scaled', 'snowdepth_scaled', 'windspeed_scaled', 'cloudcover_scaled', 'visibility_scaled', 'uvindex'], outputCol='features')\n",
    "# Ex: pizzaria_pipe = Pipeline(stages=[indexer, encoder, age_assembler, age_scaler, assembler])\n",
    "pipeline = Pipeline(stages=[indexer, encoder, temp_assembler, temp_scaler, feelslike_assembler, feelslike_scaler, humidity_assembler, humidity_scaler, precip_assembler, precip_scaler, snow_assembler, snow_scaler, snowdepth_assembler, snowdepth_scaler, windspeed_assembler, windspeed_scaler, cloudcover_assembler, cloudcover_scaler, visibility_assembler, visibility_scaler, assembler])\n",
    "transformed_df = pipeline.fit(combined_df).transform(combined_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the data frame the original columns and features column\n",
    "transformed_df.select('datetime', 'borough', 'total_trips', 'features').show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing data\n",
    "train_df, test_df = transformed_df..randomSplit([0.7, 0.3], seed=42)\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='total_trips')\n",
    "lr_model = lr.fit(train_df)\n",
    "lr_predictions = lr_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# # Show the confusion matrix for the model\n",
    "#lr_predictions.groupby('total_trips').pivot('prediction').count().sort('total_trips').show()\n",
    "confusion_matrix = lr_predictions.groupby('total_trips').pivot('prediction').count().fillna(0).collect()\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='total_trips', predictionCol='prediction')\n",
    "\n",
    "accuracy = evaluator.evaluate(lr_predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(lr_predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(lr_predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "f1_score = evaluator.evaluate(lr_predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1_score}\")\n",
    "\n",
    "# Confusion matrix\n",
    "prediction_and_labels = lr_predictions.select(\"prediction\", \"total_trips\").rdd\n",
    "metrics = MulticlassMetrics(prediction_and_labels)\n",
    "confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
