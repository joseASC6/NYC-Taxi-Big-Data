{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "from io import BytesIO\n",
    "from datetime import datetime, date\n",
    "import matplotlib.pyplot as plt\n",
    "import holidays\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, isnan, when, count, udf, to_date, year, month, date_format, size, split, dayofweek\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, MinMaxScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'my-bigdataproject-jg'\n",
    "gs_path  = f'gs://{bucket_name}/'\n",
    "cleaned_folder = 'cleaned/'\n",
    "destination_folder = 'code_and_models/'\n",
    "\n",
    "storage_client = storage.Client() \n",
    "bucket = storage_client.get_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_files = gs_path + cleaned_folder + 'weather_data_*.parquet'\n",
    "weather_df = spark.read.parquet(weather_files)\n",
    "\n",
    "\"\"\"\n",
    "Drop columns:\n",
    "tempmax\n",
    "tempmin\n",
    "feelslikemax\n",
    "feelslikemin\n",
    "preciptype\n",
    "cloudcover\n",
    "\n",
    "visibility\n",
    "humidity\n",
    "\n",
    "Keep columns:\n",
    "datetime\n",
    "borough\n",
    "\n",
    "temp\n",
    "feelslike\n",
    "precip\n",
    "snow\n",
    "snowdepth\n",
    "windspeed\n",
    "uvindex\n",
    "conditions\n",
    "\"\"\"\n",
    "\n",
    "weather_df = weather_df.drop('tempmax', 'tempmin', 'feelslikemax', 'feelslikemin', 'preciptype', 'cloudcover', 'visibility', 'humidity')\n",
    "\n",
    "weather_df = weather_df.withColumnRenamed('borough', 'weather_borough')\n",
    "weather_df = weather_df.withColumnRenamed('snow', 'snow_precip')\n",
    "\n",
    "weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi Trips DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_files = gs_path + cleaned_folder + \"taxi_data/*.parquet\"\n",
    "taxi_df = spark.read.parquet(taxi_files)\n",
    "taxi_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the number of records for each unique RatecodeID\n",
    "taxi_df.groupBy('RatecodeID').count().show()\n",
    "\n",
    "# Show the number of records for each unique passenger_count\n",
    "taxi_df.groupBy('passenger_count').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column pickup_date, which the date using the pickup_datetime column\n",
    "taxi_df = taxi_df.withColumn('pickup_date', to_date(col('pickup_datetime')))\n",
    "\n",
    "# create a new column pickup_hour, which the hour using the pickup_datetime column\n",
    "taxi_df = taxi_df.withColumn('time_of_day', \n",
    "                             when((F.hour(F.col('pickup_datetime')) >= 5) & (F.hour(F.col('pickup_datetime')) < 12), 'morning')\\\n",
    "                             .when((F.hour(F.col('pickup_datetime')) >= 12) & (F.hour(F.col('pickup_datetime')) < 21), 'afternoon')\\\n",
    "                             .otherwise('night'))\n",
    "\n",
    "\n",
    "\n",
    "# Remove the records with RatecodeID = 99 \n",
    "taxi_df = taxi_df.filter(col('RatecodeID') != 99)\n",
    "\n",
    "# Remove the records passenger_count = 0\n",
    "taxi_df = taxi_df.filter(col('passenger_count') != 0)\n",
    "\n",
    "# Remove the records with fare_amount < 3.70 (minimum fare amount)\n",
    "taxi_df = taxi_df.filter(col('fare_amount') >= 3.70)\n",
    "\n",
    "# Remove the records with total_amount <= 4.20 (minimum total amount)\n",
    "taxi_df = taxi_df.filter(col('total_amount') > 4.20)\n",
    "\n",
    "# Remove the records with trip_distance < 1/5 mile\n",
    "taxi_df = taxi_df.filter(col('trip_distance') >= 0.2)\n",
    "\n",
    "\n",
    "taxi_df = taxi_df.drop('dropoff_datetime', 'RatecodeID', 'payment_type', 'total_amount', 'pickup_datetime', 'tip_amount')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi Zone DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone_file = gs_path + cleaned_folder + 'taxi_zones_data.parquet'\n",
    "taxi_zone_df = spark.read.parquet(taxi_zone_file)\n",
    "taxi_zone_df = taxi_zone_df.drop('zone')\n",
    "taxi_zone_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the unique boroughs in the taxi_zone_df DataFrame\n",
    "taxi_zone_df.select('borough').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi data frames combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PU Location join\n",
    "taxi_df = taxi_df.join(taxi_zone_df, taxi_df.PULocationID == taxi_zone_df.LocationID, how='left')\n",
    "\n",
    "\n",
    "taxi_df = taxi_df.withColumnRenamed('Borough', 'PUBorough')\n",
    "taxi_df = taxi_df.drop('LocationID')\n",
    "\n",
    "# DO Location join\n",
    "taxi_df = taxi_df.join(taxi_zone_df, taxi_df.DOLocationID == taxi_zone_df.LocationID, how='left')\n",
    "\n",
    "taxi_df = taxi_df.withColumnRenamed('Borough', 'DOBorough')\n",
    "taxi_df = taxi_df.drop('LocationID')\n",
    "\n",
    "# Drop the PULocationID and DOLocationID columns\n",
    "taxi_df = taxi_df.drop('PULocationID', 'DOLocationID')\n",
    "\n",
    "# Drop the records where the PUBorough or DOBorough is 'EWR'\n",
    "taxi_df = taxi_df.filter((taxi_df.PUBorough != 'EWR'))\n",
    "taxi_df = taxi_df.filter((taxi_df.DOBorough != 'EWR'))\n",
    "\n",
    "taxi_df.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = taxi_df.join(weather_df, [taxi_df.pickup_date == weather_df.datetime, taxi_df.PUBorough == weather_df.weather_borough])\n",
    "\n",
    "combined_df = combined_df.drop('datetime')\n",
    "combined_df = combined_df.drop('weather_borough')\n",
    "\n",
    "combined_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the columns: tip_percentage, trip_distance, fare_amount, passenger_count\n",
    "combined_df.select('tip_percentage', 'trip_distance', 'fare_amount', 'passenger_count').summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datetime features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime\n",
    "\n",
    "# month\n",
    "combined_df = combined_df.withColumn('month', month(col('pickup_date')))\n",
    "# dayofweek\n",
    "combined_df = combined_df.withColumn('dayofweek', dayofweek(col('pickup_date')))\n",
    "\n",
    "# weekend\n",
    "combined_df = combined_df.withColumn('weekend', when(col('dayofweek') == 1, 1.0).when(col('dayofweek') == 7, 1.0).otherwise(0))\n",
    "\n",
    "# holiday\n",
    "combined_df = combined_df.withColumn('pickup_date', to_date(col('pickup_date')))\n",
    "\n",
    "# Get the min and max date in the datetime column\n",
    "min_date = combined_df.agg({\"pickup_date\": \"min\"}).collect()[0][0]\n",
    "max_date = combined_df.agg({\"pickup_date\": \"max\"}).collect()[0][0]\n",
    "\n",
    "\n",
    "# Get the holidays observed in New York\n",
    "us_holidays = holidays.UnitedStates(years=[min_date.year, max_date.year], observed=True, subdiv='NY')\n",
    "\n",
    "#print(us_holidays)\n",
    "\n",
    "# Keep only the dates of the holidays\n",
    "us_holidays = list(us_holidays.keys())\n",
    "\n",
    "# Create a new column holiday and set it to 1 if the date is a holiday, 0 otherwise\n",
    "combined_df = combined_df.withColumn('holiday', when(col('pickup_date').isin(us_holidays), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Vector Assembler Directly\n",
    "temp\n",
    "feelslike\n",
    "precip\n",
    "snow\n",
    "snowdepth\n",
    "windspeed\n",
    "uvindex\n",
    "conditions\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UDF Condition Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditions\n",
    "\n",
    "# Select all the distinct options for conditions and save them in a list\n",
    "conditions = combined_df.select('conditions').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "# Split the string into a list\n",
    "conditions = [x.split(', ') for x in conditions]\n",
    "\n",
    "# Flatten the list\n",
    "conditions = [item for sublist in conditions for item in sublist]\n",
    "# Keep the unique values only\n",
    "conditions = list(set(conditions))\n",
    "\n",
    "print(conditions)\n",
    "\n",
    "# Create a new column for each condition\n",
    "for condition in conditions:\n",
    "    combined_df = combined_df.withColumn(condition, when(col('conditions').contains(condition), 1).otherwise(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tip Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Tip Percentage to create a new column tip_class\n",
    "# If the tip percentage is greater than 10%, the tip_class is 1, otherwise 0\n",
    "combined_df = combined_df.withColumn('tip_class', when(col('tip_percentage') > 10, 1).otherwise(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String Index columns\n",
    "indexer_input = ['PUBorough', 'DOBorough', 'time_of_day']\n",
    "indexer_output = [x + '_index' for x in indexer_input]\n",
    "indexer = StringIndexer(inputCols=indexer_input, outputCols=indexer_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output = [x + '_encoded' for x in indexer_input]\n",
    "encoder = OneHotEncoder(inputCols=indexer_output, outputCols=encoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "temp\n",
    "feelslike\n",
    "precip\n",
    "snow\n",
    "snowdepth\n",
    "windspeed\n",
    "uvindex\n",
    "conditions\n",
    " 'month', 'dayofweek', 'weekend', 'holiday', 'trip_distance', 'passenger_count', 'fare_amount'\n",
    "\"\"\"\n",
    "\n",
    "encode_directly = [\n",
    "    'temp',\n",
    "    'feelslike',\n",
    "    'precip',\n",
    "    'snow_precip',\n",
    "    'snowdepth',\n",
    "    'windspeed',\n",
    "    'uvindex',\n",
    "    'month',\n",
    "    'dayofweek',\n",
    "    'weekend',\n",
    "    'holiday',\n",
    "    'trip_distance',\n",
    "    'passenger_count',\n",
    "    'fare_amount'\n",
    "]\n",
    "\n",
    "\n",
    "input_cols =  conditions + encoder_output + encode_directly\n",
    "print(input_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=input_cols, outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the pipeline with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saving the transformed data...')\n",
    "# Create a new frame with the transformed data\n",
    "pipeline = Pipeline(stages=[indexer, encoder, assembler])\n",
    "model = pipeline.fit(combined_df)\n",
    "transformed_df = model.transform(combined_df)\n",
    "\n",
    "# Save the transformed data\n",
    "transformed_df.write.parquet(gs_path + destination_folder + 'features', mode='overwrite')\n",
    "print('Transformed data saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = combined_df.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol='features', labelCol='tip_class', maxIter=10, regParam=0.1, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipeline = Pipeline(stages=[\n",
    "    indexer,\n",
    "    encoder,\n",
    "    assembler,\n",
    "    lr\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = ParamGridBuilder()\n",
    "grid = grid.addGrid(lr.regParam, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "grid = grid.addGrid(lr.elasticNetParam, [0, 0.5, 1])\n",
    "grid = grid.build()\n",
    "\n",
    "print('Number of models to be tested: ', len(grid))\n",
    "\n",
    "# Binary classification evaluator with area under ROC as the metric\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='tip_class', metricName='areaUnderROC')\n",
    "\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=lr_pipeline,\n",
    "    estimatorParamMaps=grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "all_models = cv.fit(train_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "predictions = all_models.transform(test_df)\n",
    "\n",
    "# Calculate the AUC\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"AUC: {auc}\")\n",
    "\n",
    "# Create a confusion matrix\n",
    "predictions.groupby('tip_class', 'prediction').count().show()\n",
    "cm = predictions.groupby('tip_class').pivot('prediction').count().fillna(0).collect()\n",
    "\n",
    "def calculate_metrics(cm):\n",
    "    tn = cm[0][1]                # True Negative\n",
    "    fp = cm[0][2]                # False Positive\n",
    "    fn = cm[1][1]                # False Negative\n",
    "    tp = cm[1][2]                # True Positive\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "accuracy, precision, recall, f1 = calculate_metrics(cm)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model \n",
    "best_model = all_models.bestModel\n",
    "\n",
    "print(f\"Best Model Stages: \\n{best_model.stages}\")\n",
    "\n",
    "# Parameters of the best model\n",
    "best_model.stages[-1].extractParamMap()\n",
    "\n",
    "# Create a ROC curve\n",
    "trainingSummary = best_model.stages[-1].summary\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(trainingSummary.roc.select('FPR').collect(),\n",
    "         trainingSummary.roc.select('TPR').collect())\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = best_model.stages[-1].coefficients\n",
    "print(\"bestModel coefficients\", coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "print('Saving the model')\n",
    "model_path = gs_path + destination_folder + 'model'\n",
    "best_model.write().overwrite().save(model_path)\n",
    "print('Model saved')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
