{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "from io import BytesIO\n",
    "from datetime import datetime, date\n",
    "import holidays\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, isnan, when, count, udf, to_date, year, month, date_format, size, split, dayofweek\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, MinMaxScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'my-bigdataproject-jg'\n",
    "gs_path  = f'gs://{bucket_name}/'\n",
    "cleaned_folder = 'cleaned/'\n",
    "destination_folder = 'code_and_models/'\n",
    "\n",
    "storage_client = storage.Client() \n",
    "bucket = storage_client.get_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_files = gs_path + cleaned_folder + 'weather_data_*.parquet'\n",
    "weather_df = spark.read.parquet(weather_files)\n",
    "weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi Trips DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_files = gs_path + cleaned_folder + \"taxi_data/*.parquet\"\n",
    "taxi_df = spark.read.parquet(taxi_files)\n",
    "taxi_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the number of records for each unique RatecodeID\n",
    "taxi_df.groupBy('RatecodeID').count().show()\n",
    "\n",
    "# Show the number of records for each unique passenger_count\n",
    "taxi_df.groupBy('passenger_count').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column pickup_date, which the date using the pickup_datetime column\n",
    "taxi_df = taxi_df.withColumn('pickup_date', to_date(col('pickup_datetime')))\n",
    "\n",
    "# create a new column pickup_hour, which the hour using the pickup_datetime column\n",
    "taxi_df = taxi_df.withColumn('time_of_day', \n",
    "                             when((F.hour(F.col('pickup_datetime')) >= 5) & (F.hour(F.col('pickup_datetime')) < 12), 'morning')\\\n",
    "                             .when((F.hour(F.col('pickup_datetime')) >= 12) & (F.hour(F.col('pickup_datetime')) < 21), 'afternoon')\\\n",
    "                             .otherwise('night'))\n",
    "\n",
    "# Remove the records with RatecodeID = 99 and passenger_count = 0\n",
    "taxi_df = taxi_df.filter((col('RatecodeID') != 99) & (col('passenger_count') != 0))\n",
    "\n",
    "\n",
    "\n",
    "taxi_df = taxi_df.drop('dropoff_datetime', 'RatecodeID', 'payment_type', 'total_amount')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi Zone DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone_file = gs_path + cleaned_folder + 'taxi_zones_data.parquet'\n",
    "taxi_zone_df = spark.read.parquet(taxi_zone_file)\n",
    "taxi_zone_df = taxi_zone_df.drop('zone')\n",
    "taxi_zone_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the unique boroughs in the taxi_zone_df DataFrame\n",
    "taxi_zone_df.select('borough').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi data frames combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "taxi_df = taxi_df.join(taxi_zone_df, taxi_df.PULocationID == taxi_zone_df.LocationID, how='left')\n",
    "taxi_df = taxi_df.withColumnRenamed('Borough', 'PUBorough')\n",
    "taxi_df = taxi_df.drop('LocationID')\n",
    "taxi_df = taxi_df.join(taxi_zone_df, taxi_df.DOLocationID == taxi_zone_df.LocationID, how='left')\n",
    "taxi_df = taxi_df.withColumnRenamed('Borough', 'DOBorough')\n",
    "taxi_df = taxi_df.drop('LocationID')\n",
    "taxi_df.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the records where the PUBorough or DOBorough is 'EWR'\n",
    "taxi_df = taxi_df.filter((taxi_df.PUBorough != 'EWR') & (taxi_df.DOBorough != 'EWR'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = weather_df.withColumnRenamed('borough', 'weather_borough')\n",
    "weather_df = weather_df.withColumnRenamed('snow', 'snow_precip')\n",
    "\n",
    "combined_df = taxi_df.join(weather_df, [taxi_df.pickup_date == weather_df.datetime, taxi_df.PUBorough == weather_df.weather_borough])\n",
    "\n",
    "combined_df = combined_df.drop('datetime')\n",
    "combined_df = combined_df.drop('weather_borough')\n",
    "combined_df = combined_df.drop('PULocationID', 'DOLocationID', 'pickup_datetime')\n",
    "\n",
    "combined_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show th pickup_date, time_of_day, PUBorough, DOBorough, tip_percentage, trip_distance\n",
    "combined_df.select('pickup_date', 'time_of_day', 'PUBorough', 'DOBorough', 'tip_percentage', 'trip_distance').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the columns: tip_percentage, trip_distance, fare_amount, passenger_count\n",
    "combined_df.select('tip_percentage', 'trip_distance', 'fare_amount', 'passenger_count').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datetime features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime\n",
    "\n",
    "# month\n",
    "combined_df = combined_df.withColumn('month', month(col('pickup_date')))\n",
    "# dayofweek\n",
    "combined_df = combined_df.withColumn('dayofweek', dayofweek(col('pickup_date')))\n",
    "\n",
    "# weekend\n",
    "combined_df = combined_df.withColumn('weekend', when(col('dayofweek') == 1, 1.0).when(col('dayofweek') == 7, 1.0).otherwise(0))\n",
    "\n",
    "# holiday\n",
    "combined_df = combined_df.withColumn('pickup_date', to_date(col('pickup_date')))\n",
    "\n",
    "# Get the min and max date in the datetime column\n",
    "min_date = combined_df.agg({\"pickup_date\": \"min\"}).collect()[0][0]\n",
    "max_date = combined_df.agg({\"pickup_date\": \"max\"}).collect()[0][0]\n",
    "\n",
    "\n",
    "# Get the holidays observed in New York\n",
    "us_holidays = holidays.UnitedStates(years=[min_date.year, max_date.year], observed=True, subdiv='NY')\n",
    "\n",
    "print(us_holidays)\n",
    "\n",
    "# Keep only the dates of the holidays\n",
    "us_holidays = list(us_holidays.keys())\n",
    "\n",
    "# Create a new column holiday and set it to 1 if the date is a holiday, 0 otherwise\n",
    "combined_df = combined_df.withColumn('holiday', when(col('pickup_date').isin(us_holidays), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min Max Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp\n",
    "\n",
    "# Get the min and max temp values\n",
    "temp_min = combined_df.agg(F.min('tempmin')).collect()[0][0]\n",
    "temp_max = combined_df.agg(F.max('tempmax')).collect()[0][0]\n",
    "\n",
    "temp_assembler = VectorAssembler(inputCols=['temp'], outputCol='temp_vector')\n",
    "temp_scaler = MinMaxScaler(inputCol='temp_vector', outputCol='temp_scaled', min=temp_min, max=temp_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feels_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feels_like\n",
    "\n",
    "# Get the min and max feelslike values\n",
    "feelslike_min = combined_df.agg(F.min('feelslikemin')).collect()[0][0]\n",
    "feelslike_max = combined_df.agg(F.max('feelslikemax')).collect()[0][0]\n",
    "\n",
    "feelslike_assembler = VectorAssembler(inputCols=['feelslike'], outputCol='feelslike_vector')\n",
    "feelslike_scaler = MinMaxScaler(inputCol='feelslike_vector', outputCol='feelslike_scaled', min=feelslike_min, max=feelslike_max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Min Max Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# humidity\n",
    "humidity_assembler = VectorAssembler(inputCols=['humidity'], outputCol='humidity_vector')\n",
    "humidity_scaler = MinMaxScaler(inputCol='humidity_vector', outputCol='humidity_scaled')\n",
    "\n",
    "# precip\n",
    "precip_assembler = VectorAssembler(inputCols=['precip'], outputCol='precip_vector')\n",
    "precip_scaler = MinMaxScaler(inputCol='precip_vector', outputCol='precip_scaled')\n",
    "\n",
    "# snow\n",
    "snow_precip_assembler = VectorAssembler(inputCols=['snow_precip'], outputCol='snow_precip_vector')\n",
    "snow_precip_scaler = MinMaxScaler(inputCol='snow_precip_vector', outputCol='snow_precip_scaled')\n",
    "\n",
    "# snowdepth\n",
    "snowdepth_assembler = VectorAssembler(inputCols=['snowdepth'], outputCol='snowdepth_vector')\n",
    "snowdepth_scaler = MinMaxScaler(inputCol='snowdepth_vector', outputCol='snowdepth_scaled')\n",
    "\n",
    "# windspeed\n",
    "windspeed_assembler = VectorAssembler(inputCols=['windspeed'], outputCol='windspeed_vector')\n",
    "windspeed_scaler = MinMaxScaler(inputCol='windspeed_vector', outputCol='windspeed_scaled')\n",
    "\n",
    "# cloudcover\n",
    "cloudcover_assembler = VectorAssembler(inputCols=['cloudcover'], outputCol='cloudcover_vector')\n",
    "cloudcover_scaler = MinMaxScaler(inputCol='cloudcover_vector', outputCol='cloudcover_scaled')\n",
    "\n",
    "# visibility\n",
    "visibility_assembler = VectorAssembler(inputCols=['visibility'], outputCol='visibility_vector')\n",
    "visibility_scaler = MinMaxScaler(inputCol='visibility_vector', outputCol='visibility_scaled')\n",
    "\n",
    "# uvindex\n",
    "# encode directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UDF Condition Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditions\n",
    "\n",
    "# Select all the distinct options for conditions and save them in a list\n",
    "conditions = combined_df.select('conditions').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "# Split the string into a list\n",
    "conditions = [x.split(', ') for x in conditions]\n",
    "\n",
    "# Flatten the list\n",
    "conditions = [item for sublist in conditions for item in sublist]\n",
    "# Keep the unique values only\n",
    "conditions = list(set(conditions))\n",
    "\n",
    "print(conditions)\n",
    "\n",
    "# Create a new column for each condition\n",
    "for condition in conditions:\n",
    "    combined_df = combined_df.withColumn(condition, when(col('conditions').contains(condition), 1).otherwise(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String Index columns\n",
    "indexer_input = ['PUBorough', 'DOBorough', 'time_of_day']\n",
    "indexer_output = [x + '_index' for x in indexer_input]\n",
    "indexer = StringIndexer(inputCols=indexer_input, outputCols=indexer_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output = [x + '_encoded' for x in indexer_input]\n",
    "encoder = OneHotEncoder(inputCols=indexer_output, outputCols=encoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_directly = ['uvindex', 'month', 'dayofweek', 'weekend', 'holiday', 'trip_distance', 'passenger_count', 'fare_amount']\n",
    "\n",
    "input_cols  = [\n",
    "    'temp_scaled', 'feelslike_scaled', 'humidity_scaled', 'precip_scaled', 'snow_precip_scaled', 'snowdepth_scaled', 'windspeed_scaled', 'cloudcover_scaled', 'visibility_scaled'\n",
    "]\n",
    "input_cols = input_cols + conditions + encoder_output + encode_directly\n",
    "print(input_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=input_cols, outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the pipeline with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saving the transformed data...')\n",
    "# Create a new frame with the transformed data\n",
    "pipeline = Pipeline(stages=[temp_assembler, temp_scaler, feelslike_assembler, feelslike_scaler, humidity_assembler, humidity_scaler, precip_assembler, precip_scaler, snow_precip_assembler, snow_precip_scaler, snowdepth_assembler, snowdepth_scaler, windspeed_assembler, windspeed_scaler, cloudcover_assembler, cloudcover_scaler, visibility_assembler, visibility_scaler, indexer, encoder, assembler])\n",
    "model = pipeline.fit(combined_df)\n",
    "transformed_df = model.transform(combined_df)\n",
    "\n",
    "# Save the transformed data\n",
    "transformed_df.write.parquet(gs_path + destination_folder + 'features')\n",
    "print('Transformed data saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Test Regression Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = combined_df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "linear_reg = LinearRegression(featuresCol='features', labelCol='tip_percentage')\n",
    "evaluator = RegressionEvaluator(labelCol='tip_percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regresion_pipe = Pipeline(stages=[\n",
    "    indexer, temp_assembler, temp_scaler, feelslike_assembler, feelslike_scaler, humidity_assembler, humidity_scaler, precip_assembler, precip_scaler, snow_precip_assembler, snow_precip_scaler, snowdepth_assembler, snowdepth_scaler, windspeed_assembler, windspeed_scaler, cloudcover_assembler, cloudcover_scaler, visibility_assembler, visibility_scaler, encoder, assembler, linear_reg\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = ParamGridBuilder()\n",
    "grid = grid.build()\n",
    "\n",
    "cv = CrossValidator(estimator=regresion_pipe, estimatorParamMaps=grid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "all_models = cv.fit(train_df)\n",
    "\n",
    "print(f\"Average metrics: {all_models.avgMetrics}\")\n",
    "\n",
    "best_model = all_models.bestModel\n",
    "\n",
    "test_results = best_model.transform(test_df)\n",
    "\n",
    "rmse = evaluator.evaluate(test_results, {evaluator.metricName: \"rmse\"})\n",
    "mse = evaluator.evaluate(test_results, {evaluator.metricName: \"mse\"})\n",
    "mae = evaluator.evaluate(test_results, {evaluator.metricName: \"mae\"})\n",
    "r2 = evaluator.evaluate(test_results, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R2: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = best_model.stages[-1].coefficients\n",
    "print(\"bestModel coefficients\", coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "print('Saving the model')\n",
    "model_path = gs_path + destination_folder + 'model'\n",
    "best_model.write().overwrite().save(model_path)\n",
    "print('Model saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol='borough', outputCol='borough_index')\n",
    "encoder = OneHotEncoder(inputCol='borough_index', outputCol='borough_encoded')\n",
    "assembler = VectorAssembler(inputCols=['month', 'dayofweek', 'weekend', 'borough_encoded', 'temp_scaled', 'feelslike_scaled', 'humidity_scaled', 'precip_scaled', 'snow_scaled', 'snowdepth_scaled', 'windspeed_scaled', 'cloudcover_scaled', 'visibility_scaled', 'uvindex'], outputCol='features')\n",
    "\n",
    "train_df, test_df = combined_df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "linear_reg = LinearRegression(labelCol='total_trips')\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='total_trips')\n",
    "\n",
    "regresion_pipe = Pipeline(stages=[indexer, encoder, temp_assembler, temp_scaler, feelslike_assembler, feelslike_scaler, humidity_assembler, humidity_scaler, precip_assembler, precip_scaler, snow_precip_assembler, snow_precip_scaler, snowdepth_assembler, snowdepth_scaler, windspeed_assembler, windspeed_scaler, cloudcover_assembler, cloudcover_scaler, visibility_assembler, visibility_scaler, assembler, linear_reg])\n",
    "\n",
    "grid = ParamGridBuilder()\n",
    "grid = grid.build()\n",
    "cv = CrossValidator(estimator=regresion_pipe, estimatorParamMaps=grid, evaluator=evaluator, numFolds=3)\n",
    "all_models = cv.fit(train_df)\n",
    "\n",
    "print(f\"Average metrics: {all_models.avgMetrics}\")\n",
    "\n",
    "best_model = all_models.bestModel\n",
    "\n",
    "test_results = best_model.transform(test_df)\n",
    "\n",
    "rmse = evaluator.evaluate(test_results, {evaluator.metricName: \"rmse\"})\n",
    "mse = evaluator.evaluate(test_results, {evaluator.metricName: \"mse\"})\n",
    "mae = evaluator.evaluate(test_results, {evaluator.metricName: \"mae\"})\n",
    "r2 = evaluator.evaluate(test_results, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R2: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol='borough', outputCol='borough_index')\n",
    "encoder = OneHotEncoder(inputCol='borough_index', outputCol='borough_encoded')\n",
    "assembler = VectorAssembler(inputCols=['month', 'dayofweek', 'weekend', 'borough_encoded', 'temp_scaled', 'feelslike_scaled', 'humidity_scaled', 'precip_scaled', 'snow_scaled', 'snowdepth_scaled', 'windspeed_scaled', 'cloudcover_scaled', 'visibility_scaled', 'uvindex'], outputCol='features')\n",
    "\n",
    "pipeline = Pipeline(stages=[temp_assembler, temp_scaler, feelslike_assembler, feelslike_scaler, humidity_assembler, humidity_scaler, precip_assembler, precip_scaler, snow_precip_assembler, snow_precip_scaler, snowdepth_assembler, snowdepth_scaler, windspeed_assembler, windspeed_scaler, cloudcover_assembler, cloudcover_scaler, visibility_assembler, visibility_scaler, indexer, encoder, assembler])\n",
    "\n",
    "pipeline_model = pipeline.fit(combined_df).transform(combined_df)\n",
    "\n",
    "train_df, test_df = pipeline_model.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "lr = LinearRegression(featuresCol='features', labelCol='total_trips')\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "lr_predictions = lr_model.transform(test_df)\n",
    "lr_evaluator = RegressionEvaluator(predictionCol='prediction', labelCol='total_trips')\n",
    "\n",
    "rmse = lr_evaluator.evaluate(lr_predictions, {lr_evaluator.metricName: 'rmse'})\n",
    "mae = lr_evaluator.evaluate(lr_predictions, {lr_evaluator.metricName: 'mae'})\n",
    "mse = lr_evaluator.evaluate(lr_predictions, {lr_evaluator.metricName: 'mse'})\n",
    "r2 = lr_evaluator.evaluate(lr_predictions, {lr_evaluator.metricName: 'r2'})\n",
    "\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'MSE: {mse}')\n",
    "print(f'R2: {r2}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
